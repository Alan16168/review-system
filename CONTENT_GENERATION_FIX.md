# 内容生成优化文档

## 问题概述

用户报告了两个主要问题：

1. **字数超标100%**: 设定1000字，实际生成2535字（超出154%）
2. **内容不完整**: 生成的内容像写了一半，没有完整结束

## 问题分析

### 问题 1: 字数超标

**原因**:
```typescript
// 错误的 token 计算公式
const estimatedTokens = Math.ceil(targetWords * 2.5);

// 问题:
// - 中文 1个字 ≈ 2-3 tokens (平均 2.5)
// - 但这里的逻辑反了：2500 tokens 会生成约 2000-2500 个中文字
// - 实际上应该是: 1000 字需要 1000 / 0.6 ≈ 1667 tokens
```

**根本错误**:
- 误认为 1个中文字 = 2.5 tokens
- 实际上 1 token ≈ 0.5-0.7 个中文字（平均 0.6）
- 所以 `targetWords * 2.5` 会生成远超预期的字数

### 问题 2: 内容不完整

**原因**:
1. **maxTokens 限制**: 计算出的 tokens 可能触达上限，导致内容被截断
2. **Prompt 不明确**: 没有强调"完整结束"，AI 可能在任意位置停止
3. **没有后处理**: 缺少检查内容完整性的机制

## 解决方案

### 1. 修正 Token 计算公式

**之前（错误）**:
```typescript
// 1000 字 * 2.5 = 2500 tokens
// 实际会生成约 2000-2500 字（超标100%+）
const estimatedTokens = Math.ceil(targetWords * 2.5);
```

**现在（正确）**:
```typescript
// 1000 字 / 0.6 * 1.2 = 2000 tokens
// - 1 token ≈ 0.6 个中文字
// - 1.2 是缓冲系数（20%），用于 markdown 格式和完整性
// 实际会生成约 900-1100 字（±10%）
const estimatedTokens = Math.ceil((targetWords / 0.6) * 1.2);
```

**Token vs 字数对照表**:

| 目标字数 | 旧公式 Tokens | 新公式 Tokens | 预期实际字数 |
|---------|--------------|--------------|------------|
| 500字   | 1,250        | 1,000        | 450-550    |
| 1000字  | 2,500        | 2,000        | 900-1100   |
| 2000字  | 5,000        | 4,000        | 1800-2200  |
| 5000字  | 12,500       | 10,000       | 4500-5500  |

### 2. 优化 Prompt 设计

#### 之前的 Prompt 问题:
- ❌ 只是"约"字数，没有范围限制
- ❌ 没有强调完整性
- ❌ 允许"如果内容不够，请继续扩展"（导致超标）

#### 现在的 Prompt 改进:

```
【核心任务】
请为这个小节生成${targetWords}字左右的完整内容（允许误差±10%，即900-1100字）。

【内容要求】
1. ⚠️ 字数控制：生成内容必须在900-1100字范围内（不包含markdown标记符号）
2. 结构完整：内容必须有完整的开头、正文和结尾，不能突然中断
3. 格式规范：使用Markdown格式

【特别要求】
- ✅ 必须有明确的结论或总结段落
- ✅ 内容要完整，不能戛然而止
- ✅ 如果接近字数上限，要用简洁的方式收尾
- ❌ 不要包含"未完待续"、"下一节将"等字样
- ❌ 不要超出规定字数范围
```

**关键改进**:
1. **明确字数范围** (±10%)
2. **强调完整性** （开头、正文、结尾）
3. **禁止截断** （不要"未完待续"）
4. **智能收尾** （接近上限时简洁结尾）

### 3. 实现内容完整性检查

```typescript
// 检查内容是否不完整
const incompleteIndicators = [
  /[，、：；][^。！？\n]*$/,  // 以逗号、冒号结尾
  /^\s*[\d一二三四五六七八九十]+[\.\)、]/m,  // 以列表项结尾
  /\*\*[^*]+$/,  // 未闭合的粗体标记
  /```[^`]*$/  // 未闭合的代码块
];

const seemsIncomplete = incompleteIndicators.some(pattern => 
  pattern.test(content.trim())
);

// 如果不完整，自动补充结尾
if (wordCount < targetWords * 0.85 || seemsIncomplete) {
  // 调用 API 生成简短的结尾段落（约200字）
  const conclusion = await callGeminiAPI(conclusionPrompt, 500);
  content = content + '\n\n' + conclusion;
}
```

**完整性检查逻辑**:
1. 检查是否以不完整的标点结尾
2. 检查是否以列表项结尾
3. 检查是否有未闭合的 markdown 标记
4. 字数少于目标85%也视为不完整
5. 自动生成简短结尾（约200字）

### 4. 改进字数统计

**之前**:
```typescript
// 只排除空格
const wordCount = content.replace(/\s/g, '').length;
```

**现在**:
```typescript
// 排除空格和 markdown 符号
const wordCount = content
  .replace(/\s/g, '')  // 空格
  .replace(/[#*\->`\[\]()]/g, '')  // markdown 符号
  .length;
```

**更准确的字数统计**:
- ✅ 排除空格
- ✅ 排除标题符号 (#)
- ✅ 排除粗体/斜体符号 (*, **)
- ✅ 排除列表符号 (-, >, `)
- ✅ 排除链接/图片符号 ([], ())

## 测试结果

### 测试用例 1: 1000字内容生成

**设定**:
```json
{
  "target_word_count": 1000
}
```

**之前的结果**:
```
实际字数: 2535字
超出比例: +154%
完整性: ❌ 不完整（突然中断）
```

**现在的预期**:
```
实际字数: 900-1100字
误差范围: ±10%
完整性: ✅ 完整（有结尾）
```

### 测试用例 2: 2000字内容生成

**设定**:
```json
{
  "target_word_count": 2000
}
```

**预期结果**:
```
实际字数: 1800-2200字
Token 使用: ~4000 tokens
完整性: ✅ 完整
格式: ✅ Markdown
```

## 技术细节

### Token 计算原理

**中文 Token 特性**:
- Gemini 使用 BPE (Byte Pair Encoding) tokenizer
- 中文字符通常需要 1.5-2 tokens
- 平均: 1 token ≈ 0.6 个中文字

**计算公式推导**:
```
目标字数: N
需要的 tokens: N / 0.6 = 1.67N
加 20% 缓冲: 1.67N * 1.2 = 2N

示例:
1000 字 → 1000 / 0.6 * 1.2 = 2000 tokens
2000 字 → 2000 / 0.6 * 1.2 = 4000 tokens
```

### 字数范围控制

**±10% 策略**:
```
目标: 1000字
允许范围: 900-1100字
计算: Math.floor(1000 * 0.9) - Math.ceil(1000 * 1.1)
```

**为什么 ±10%?**
- ✅ 给 AI 一定的弹性空间
- ✅ 避免为了凑字数而生成低质量内容
- ✅ 确保内容能够自然完整结尾
- ✅ 行业标准做法

### 完整性保障机制

**三重保障**:

1. **Prompt 层面**:
   - 明确要求完整的结构
   - 禁止"未完待续"等字样
   - 强调必须有结尾

2. **检测层面**:
   - 正则表达式检测不完整标志
   - 字数阈值检查（< 85%）
   - Markdown 标记闭合检查

3. **修复层面**:
   - 自动生成简短结尾（约200字）
   - 总结核心要点
   - 给出实践建议或展望

## 使用指南

### 推荐字数设置

| 内容类型 | 推荐字数 | Token 使用 | 生成时间 |
|---------|---------|-----------|---------|
| 简短说明 | 500字   | ~1000     | 5-10秒  |
| 标准小节 | 1000字  | ~2000     | 10-15秒 |
| 详细章节 | 2000字  | ~4000     | 15-25秒 |
| 深度内容 | 5000字  | ~10000    | 30-60秒 |

### 最佳实践

1. **字数设置**:
   - 小节内容: 1000-2000字
   - 概述性内容: 500-800字
   - 深度分析: 2000-3000字

2. **生成策略**:
   - 首次生成: 使用默认1000字
   - 内容太短: 点击"重新生成"，提高字数
   - 内容太长: 点击"编辑"手动精简

3. **质量优先**:
   - 不要为了字数而字数
   - 允许±10%的误差是合理的
   - 完整性比精确字数更重要

## 监控和日志

### 日志输出示例

```
Generating content: target=1000 words, estimated tokens=2000, max tokens=2000, system max=8192

Generated content: 956 words (target: 1000, range: 900-1100)

✅ Content complete and within range
```

### 异常情况

```
Generated content: 756 words (target: 1000, range: 900-1100)

⚠️ Content seems incomplete (756 words, incomplete indicators: true), adding conclusion...

After adding conclusion: 983 words

✅ Content completed with automatic conclusion
```

## 已知限制

1. **最大字数限制**:
   - System max tokens: 8192
   - 理论最大字数: ~4900 字
   - 实际建议上限: 5000 字

2. **质量 vs 字数权衡**:
   - 过长的内容可能质量下降
   - 建议分成多个小节

3. **语言差异**:
   - 当前优化针对中文
   - 英文字数计算可能不准确
   - Token 比例不同: 1 token ≈ 0.75 个英文单词

## 未来优化方向

1. **动态 Token 调整**:
   - 根据实际生成结果调整公式
   - 机器学习优化 token 系数

2. **多语言支持**:
   - 自动检测语言
   - 使用不同的 token 系数

3. **智能字数预测**:
   - 分析历史生成数据
   - 预测最优 token 数量

4. **质量评分**:
   - 评估内容完整性
   - 评估内容质量
   - 自动重试低质量内容

---

**更新日期**: 2025-11-20  
**Git Commit**: d42fd91  
**影响范围**: AI 内容生成功能  
**测试状态**: ✅ 已部署，待验证
